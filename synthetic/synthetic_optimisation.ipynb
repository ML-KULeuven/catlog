{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a synthetic optimisation experiment. Specifically, we try to maximise the expectation\n",
    "$$\n",
    "\\mathbb{E}_{\\mathbf{X} \\sim p(\\mathbf{X})}\\left[\n",
    "    \\frac{1}{D}\\sum_{i = 1}^D (X_i - 0.499)^2\n",
    "\\right],\n",
    "$$\n",
    "using different gradient estimators. We choose to set $D = 200$. While being a synthetic example, this optimisation task is already challenging because of the small impact of on the overal expectation value of changing a single variable.\n",
    "We optimised the hyperparameters for each method, which are the following for each method.\n",
    "\n",
    "1. `LR = 5.` for IndeCateR ('icr') with `SAMPLES = 2`\n",
    "2. `LR = 5.` for RLOO ('rloo')' with `SAMPLES = 800` (RLOO-F)\n",
    "3. `LR = 1.` for RLOO ('rloo')' with `SAMPLES = 2` (RLOO-S)\n",
    "4. `LR = 0.01` for Gumbel-Softmax ('gs) with `SAMPLES = 800`, `TEMP = 0.1` and `ANNEAL_RATE = 0.05`. We only compare Gumbel-Softmax with 800 samples, because it was already very unstable because of the challening problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:08:53.884360801Z",
     "start_time": "2023-12-11T14:08:53.882449563Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from losses import sq_loss\n",
    "from jointcategorical import JointCategorical\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "DIM = 200\n",
    "CATS = 2\n",
    "SAMPLES = 2\n",
    "TEMP = 0.1\n",
    "ANNEAL_RATE = 0.05\n",
    "LR = 5.\n",
    "OPTIM = 'rms'\n",
    "\n",
    "### RLOO-800 and ICR LR 5\n",
    "### RLOO-2 LR 1.0\n",
    "### GS LR 0.01 and TEMP 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs 2000 iterations of optimisations using the desired gradient type and stores the training results for 10 independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:09:01.785723086Z",
     "start_time": "2023-12-11T14:08:53.885822705Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 15:08:53.915542: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-12-11 15:08:53.915575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ZBook-LennertDTAI\n",
      "2023-12-11 15:08:53.915579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ZBook-LennertDTAI\n",
      "2023-12-11 15:08:53.915702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.147.5\n",
      "2023-12-11 15:08:53.915719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.147.5\n",
      "2023-12-11 15:08:53.915722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.147.5\n",
      "2023-12-11 15:08:53.916116: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: tf.Tensor(-0.25000602, shape=(), dtype=float32)\n",
      "Iterations 5: mean function -0.25013601779937744 Time (s): 0.018177509307861328 Gradient variance: 3.153829122049904e-12\n",
      "Iterations 10: mean function -0.2502550184726715 Time (s): 0.011396408081054688 Gradient variance: 3.4891495075128898e-12\n",
      "Iterations 15: mean function -0.2504020035266876 Time (s): 0.012005329132080078 Gradient variance: 3.0775961206569447e-12\n",
      "Iterations 20: mean function -0.25049400329589844 Time (s): 0.013675689697265625 Gradient variance: 2.6490784392485534e-12\n",
      "Iterations 25: mean function -0.250588983297348 Time (s): 0.012252092361450195 Gradient variance: 2.2703594646650282e-12\n",
      "Iterations 30: mean function -0.25062698125839233 Time (s): 0.012575864791870117 Gradient variance: 1.953133835219667e-12\n",
      "Iterations 35: mean function -0.2506759762763977 Time (s): 0.011400222778320312 Gradient variance: 1.6881945060637227e-12\n",
      "Iterations 40: mean function -0.2507140040397644 Time (s): 0.01153874397277832 Gradient variance: 1.4812393932944756e-12\n",
      "Iterations 45: mean function -0.2507459819316864 Time (s): 0.014807701110839844 Gradient variance: 1.3067820480230918e-12\n",
      "Iterations 50: mean function -0.2507990002632141 Time (s): 0.012047052383422852 Gradient variance: 1.165319143264787e-12\n",
      "Iterations 55: mean function -0.2508009970188141 Time (s): 0.011972427368164062 Gradient variance: 1.0468334098873155e-12\n",
      "Iterations 60: mean function -0.2507990002632141 Time (s): 0.010588645935058594 Gradient variance: 9.470237094522105e-13\n",
      "Iterations 65: mean function -0.2508150041103363 Time (s): 0.012116670608520508 Gradient variance: 8.65468558364918e-13\n",
      "Iterations 70: mean function -0.25082701444625854 Time (s): 0.011908769607543945 Gradient variance: 7.927150147239714e-13\n",
      "Iterations 75: mean function -0.2508299946784973 Time (s): 0.012993574142456055 Gradient variance: 7.307096008997427e-13\n",
      "Iterations 80: mean function -0.25085702538490295 Time (s): 0.01190638542175293 Gradient variance: 6.764598646860631e-13\n",
      "Iterations 85: mean function -0.25086498260498047 Time (s): 0.011252641677856445 Gradient variance: 6.288449578770172e-13\n",
      "Iterations 90: mean function -0.25086697936058044 Time (s): 0.010449409484863281 Gradient variance: 5.88133383140621e-13\n",
      "Iterations 95: mean function -0.2508789896965027 Time (s): 0.011558294296264648 Gradient variance: 5.508015376264053e-13\n",
      "Iterations 100: mean function -0.2508949935436249 Time (s): 0.011397838592529297 Gradient variance: 5.170181774025173e-13\n",
      "Iterations 105: mean function -0.25088101625442505 Time (s): 0.011749982833862305 Gradient variance: 4.873872572891402e-13\n",
      "Iterations 110: mean function -0.250901997089386 Time (s): 0.011064767837524414 Gradient variance: 4.611804102667982e-13\n",
      "Iterations 115: mean function -0.25090599060058594 Time (s): 0.012746572494506836 Gradient variance: 4.364227349732547e-13\n",
      "Iterations 120: mean function -0.25090399384498596 Time (s): 0.011647701263427734 Gradient variance: 4.135413257493059e-13\n",
      "Iterations 125: mean function -0.25092998147010803 Time (s): 0.012386322021484375 Gradient variance: 3.9319212490930566e-13\n",
      "Iterations 130: mean function -0.2509259581565857 Time (s): 0.011432886123657227 Gradient variance: 3.744029825752948e-13\n",
      "Iterations 135: mean function -0.25091201066970825 Time (s): 0.011558055877685547 Gradient variance: 3.578830482832418e-13\n",
      "Iterations 140: mean function -0.250931978225708 Time (s): 0.012814044952392578 Gradient variance: 3.40385001001281e-13\n",
      "Iterations 145: mean function -0.25092896819114685 Time (s): 0.012051820755004883 Gradient variance: 3.255335183274116e-13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitial loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, joint\u001B[38;5;241m.\u001B[39mgrads(sq_loss, \u001B[38;5;28;01mNone\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m      6\u001B[0m joint\u001B[38;5;241m.\u001B[39moptimiser \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mRMSprop(LR)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mjoint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msq_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_its\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m grad \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     10\u001B[0m     pickle\u001B[38;5;241m.\u001B[39mdump(joint\u001B[38;5;241m.\u001B[39mlogger, \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults_s2/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrad\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_temp\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTEMP\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_a\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mANNEAL_RATE\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_s\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSAMPLES\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_d\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDIM\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_o\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mOPTIM\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_lr\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mLR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseed\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_nonorm.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m))    \n",
      "File \u001B[0;32m~/Documents/PycharmProjects/catlog/synthetic/jointcategorical.py:138\u001B[0m, in \u001B[0;36mJointCategorical.train\u001B[0;34m(self, iterations, loss, target, log_its, learning_rate)\u001B[0m\n\u001B[1;32m    136\u001B[0m var_grads\u001B[38;5;241m.\u001B[39mappend(tf\u001B[38;5;241m.\u001B[39mreduce_mean([tf\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mreduce_variance(g) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m grads]))\n\u001B[1;32m    137\u001B[0m prev_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 138\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m it \u001B[38;5;241m%\u001B[39m log_its \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    140\u001B[0m     acc_time \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m prev_time\n",
      "File \u001B[0;32m~/Documents/PythonEnvironments/DTAI1_3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1139\u001B[0m, in \u001B[0;36mOptimizer.apply_gradients\u001B[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001B[0m\n\u001B[1;32m   1135\u001B[0m experimental_aggregate_gradients \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexperimental_aggregate_gradients\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m )\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m skip_gradients_aggregation \u001B[38;5;129;01mand\u001B[39;00m experimental_aggregate_gradients:\n\u001B[0;32m-> 1139\u001B[0m     grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads_and_vars\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mapply_gradients(grads_and_vars, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[0;32m~/Documents/PythonEnvironments/DTAI1_3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1105\u001B[0m, in \u001B[0;36mOptimizer.aggregate_gradients\u001B[0;34m(self, grads_and_vars)\u001B[0m\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maggregate_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m, grads_and_vars):\n\u001B[1;32m   1094\u001B[0m     \u001B[38;5;124;03m\"\"\"Aggregate gradients on all devices.\u001B[39;00m\n\u001B[1;32m   1095\u001B[0m \n\u001B[1;32m   1096\u001B[0m \u001B[38;5;124;03m    By default we will perform reduce_sum of gradients across devices. Users\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;124;03m      List of (gradient, variable) pairs.\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_reduce_sum_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads_and_vars\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PythonEnvironments/DTAI1_3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py:37\u001B[0m, in \u001B[0;36mall_reduce_sum_gradients\u001B[0;34m(grads_and_vars)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mstrategy_supports_no_merge_call():\n\u001B[1;32m     36\u001B[0m     grads \u001B[38;5;241m=\u001B[39m [pair[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m pair \u001B[38;5;129;01min\u001B[39;00m filtered_grads_and_vars]\n\u001B[0;32m---> 37\u001B[0m     reduced \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_replica_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_reduce\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mReduceOp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSUM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrads\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# TODO(b/183257003): Remove this branch\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     reduced \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_replica_context()\u001B[38;5;241m.\u001B[39mmerge_call(\n\u001B[1;32m     43\u001B[0m         _all_reduce_sum_fn, args\u001B[38;5;241m=\u001B[39m(filtered_grads_and_vars,)\n\u001B[1;32m     44\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/PythonEnvironments/DTAI1_3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3269\u001B[0m, in \u001B[0;36mReplicaContextBase.all_reduce\u001B[0;34m(self, reduce_op, value, options)\u001B[0m\n\u001B[1;32m   3263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_indexed_slices:\n\u001B[1;32m   3264\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m nest\u001B[38;5;241m.\u001B[39mpack_sequence_as(\n\u001B[1;32m   3265\u001B[0m       value,\n\u001B[1;32m   3266\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerge_call(batch_all_reduce, args\u001B[38;5;241m=\u001B[39mflattened_value))\n\u001B[1;32m   3268\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;129;43m@custom_gradient\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcustom_gradient\u001B[49m\n\u001B[0;32m-> 3269\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43mgrad_wrapper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mxs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3270\u001B[0m \u001B[43m  \u001B[49m\u001B[43mys\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_all_reduce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3271\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# The gradient of an all-sum is itself an all-sum (all-mean, likewise).\u001B[39;49;00m\n",
      "File \u001B[0;32m~/Documents/PythonEnvironments/DTAI1_3.8/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py:300\u001B[0m, in \u001B[0;36mcustom_gradient\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m    297\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _graph_mode_decorator(wrapped, args, kwargs)\n\u001B[0;32m--> 300\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf_decorator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_decorator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecorated\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PythonEnvironments/DTAI1_3.8/lib/python3.8/site-packages/tensorflow/python/util/tf_decorator.py:81\u001B[0m, in \u001B[0;36mmake_decorator\u001B[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;124;03m\"\"\"Make a decorator from a wrapper and a target.\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \n\u001B[1;32m     68\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m  The `decorator_func` argument with new metadata attached.\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decorator_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m   decorator_name \u001B[38;5;241m=\u001B[39m \u001B[43minspect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrentframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mf_back\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\n\u001B[1;32m     82\u001B[0m decorator \u001B[38;5;241m=\u001B[39m TFDecorator(decorator_name, target, decorator_doc,\n\u001B[1;32m     83\u001B[0m                         decorator_argspec)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28msetattr\u001B[39m(decorator_func, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_tf_decorator\u001B[39m\u001B[38;5;124m'\u001B[39m, decorator)\n",
      "File \u001B[0;32m/usr/lib/python3.8/inspect.py:1520\u001B[0m, in \u001B[0;36mcurrentframe\u001B[0;34m()\u001B[0m\n\u001B[1;32m   1517\u001B[0m         tb \u001B[38;5;241m=\u001B[39m tb\u001B[38;5;241m.\u001B[39mtb_next\n\u001B[1;32m   1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m framelist\n\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcurrentframe\u001B[39m():\n\u001B[1;32m   1521\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\u001B[39;00m\n\u001B[1;32m   1522\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sys\u001B[38;5;241m.\u001B[39m_getframe(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(sys, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_getframe\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for grad in ['icr']:\n",
    "    for seed in range(10):\n",
    "        joint = JointCategorical(DIM, CATS, SAMPLES, anneal_rate=ANNEAL_RATE, temp=TEMP, grad_type=grad)\n",
    "\n",
    "        print(\"Initial loss:\", joint.grads(sq_loss, None)[0])\n",
    "        joint.optimiser = tf.keras.optimizers.RMSprop(LR)\n",
    "        joint.train(2000, loss=sq_loss, target=None, log_its=5)\n",
    "\n",
    "        if grad == 'gs':\n",
    "            pickle.dump(joint.logger, open(f\"results_s2/{grad}_temp{TEMP}_a{ANNEAL_RATE}_s{SAMPLES}_d{DIM}_o{OPTIM}_lr{LR}_{seed}_nonorm.pkl\", \"wb\"))    \n",
    "        pickle.dump(joint.logger, open(f\"results_s2/{grad}_s{SAMPLES}_d{DIM}_o{OPTIM}_lr{LR}_{seed}_nonorm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:09:01.787015399Z",
     "start_time": "2023-12-11T14:09:01.786652390Z"
    }
   },
   "outputs": [],
   "source": [
    "red_salsa = '#F94144'  # main red colour\n",
    "orange_red = '#F3722C'\n",
    "yellow_orange= '#F8961E'\n",
    "mango_tango = '#F9844A'\n",
    "maize_crayola = '#F9C74F'  # yellowish\n",
    "pistachio = '#90BE6D'  # greenish colour\n",
    "jungle_green = '#43AA8B'\n",
    "steel_teal = '#4D908E'\n",
    "queen_blue = '#577590'\n",
    "celadon_blue = '#277DA1'  # main blue colour\n",
    "pink = '#FA39FA'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all configurations are optimised, the next couple of cells load the stored optimisation results. These results, being expected function value and gradient variance throughout training, can then be plotted in function of time and iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:09:01.788945462Z",
     "start_time": "2023-12-11T14:09:01.788764981Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAD = 'rloo'\n",
    "SAMPLES = 800\n",
    "\n",
    "train_losses = []\n",
    "variances = []\n",
    "time = []\n",
    "for i in range(10):\n",
    "    logger = pickle.load(open(f\"results_s2/{GRAD}_s{SAMPLES}_d{DIM}_o{OPTIM}_lr{5.}_{i}_nonorm.pkl\", \"rb\"))\n",
    "    train_losses.append(list(logger.log_dict[\"training_loss\"].values()))\n",
    "    variances.append(list(logger.log_dict[\"gradient_variance\"].values()))\n",
    "    time.append(list(logger.log_dict[\"time\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:09:01.789697118Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAD = 'icr'\n",
    "\n",
    "train_losses2 = []\n",
    "variances2 = []\n",
    "time2 = []\n",
    "for i in range(10):\n",
    "    logger = pickle.load(open(f\"results_s2/{GRAD}_s{2}_d{DIM}_o{OPTIM}_lr{5.}_{i}_nonorm.pkl\", \"rb\"))\n",
    "    train_losses2.append(list(logger.log_dict[\"training_loss\"].values()))\n",
    "    variances2.append(list(logger.log_dict[\"gradient_variance\"].values()))\n",
    "    time2.append(list(logger.log_dict[\"time\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:09:01.790424491Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAD = 'rloo'\n",
    "\n",
    "train_losses4 = []\n",
    "variances4 = []\n",
    "time4 = []\n",
    "for i in range(10):\n",
    "    logger = pickle.load(open(f\"results_s2/{GRAD}_s{2}_d{DIM}_o{OPTIM}_lr{1.}_{i}_nonorm.pkl\", \"rb\"))\n",
    "    train_losses4.append(list(logger.log_dict[\"training_loss\"].values()))\n",
    "    variances4.append(list(logger.log_dict[\"gradient_variance\"].values()))\n",
    "    time4.append(list(logger.log_dict[\"time\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:09:01.838635129Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAD = 'gs'\n",
    "TEMP = 0.1\n",
    "\n",
    "train_losses3 = []\n",
    "variances3 = []\n",
    "time3 = []\n",
    "for i in range(10):\n",
    "    logger = pickle.load(open(f\"results_s2/{GRAD}_temp{TEMP}_a{ANNEAL_RATE}_s800_d200_orms_lr{1e-2}_{i}_nonorm.pkl\", \"rb\"))\n",
    "    train_losses3.append(list(logger.log_dict[\"training_loss\"].values()))\n",
    "    variances3.append(list(logger.log_dict[\"gradient_variance\"].values()))\n",
    "    time3.append(list(logger.log_dict[\"time\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:09:01.838979028Z"
    }
   },
   "outputs": [],
   "source": [
    "train_avg = -tf.reduce_mean(train_losses, axis=0)\n",
    "train_std = tf.math.reduce_std(train_losses, axis=0) / np.sqrt(10)\n",
    "train_avg2 = -tf.reduce_mean(train_losses2, axis=0)\n",
    "train_std2 = tf.math.reduce_std(train_losses2, axis=0) / np.sqrt(10)\n",
    "train_avg3 = -tf.reduce_mean(train_losses3, axis=0)\n",
    "train_std3 = tf.math.reduce_std(train_losses3, axis=0) / np.sqrt(10)\n",
    "train_avg4 = -tf.reduce_mean(train_losses4, axis=0)\n",
    "train_std4 = tf.math.reduce_std(train_losses4, axis=0) / np.sqrt(10)\n",
    "its = [i * 5 for i in range(len(train_avg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:09:01.856963504Z",
     "start_time": "2023-12-11T14:09:01.839227980Z"
    }
   },
   "outputs": [],
   "source": [
    "train_avg = -tf.reduce_mean(train_losses, axis=0)\n",
    "train_std = tf.math.reduce_std(train_losses, axis=0) / np.sqrt(10)\n",
    "train_avg2 = -tf.reduce_mean(train_losses2, axis=0)\n",
    "train_std2 = tf.math.reduce_std(train_losses2, axis=0) / np.sqrt(10)\n",
    "train_avg3 = -tf.reduce_mean(train_losses3, axis=0)\n",
    "train_std3 = tf.math.reduce_std(train_losses3, axis=0) / np.sqrt(10)\n",
    "train_avg4 = -tf.reduce_mean(train_losses4, axis=0)\n",
    "train_std4 = tf.math.reduce_std(train_losses4, axis=0) / np.sqrt(10)\n",
    "\n",
    "its = tf.range(len(train_avg), dtype=tf.float32)\n",
    "its1 = its * np.mean(np.mean(time, axis=0), axis=0)\n",
    "its2 = its * np.mean(np.mean(time2, axis=0), axis=0)\n",
    "its3 = its * np.mean(np.mean(time3, axis=0), axis=0)\n",
    "its4 = its * np.mean(np.mean(time4, axis=0), axis=0)\n",
    "\n",
    "min_time = tf.reduce_min([its1[-1], its2[-1], its3[-1], its4[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:09:01.839391913Z"
    }
   },
   "outputs": [],
   "source": [
    "var_avg = tf.reduce_mean(variances, axis=0)\n",
    "var_std = tf.math.reduce_std(variances, axis=0) / np.sqrt(10)\n",
    "var_avg2 = tf.reduce_mean(variances2, axis=0)\n",
    "var_std2 = tf.math.reduce_std(variances2, axis=0) / np.sqrt(10)\n",
    "var_avg3 = tf.reduce_mean(variances3, axis=0)\n",
    "var_std3 = tf.math.reduce_std(variances3, axis=0) / np.sqrt(10)\n",
    "var_avg4 = tf.reduce_mean(variances4, axis=0)\n",
    "var_std4 = tf.math.reduce_std(variances4, axis=0) / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:09:01.839531205Z"
    }
   },
   "outputs": [],
   "source": [
    "its = [i * 5 for i in range(len(train_avg))]\n",
    "\n",
    "fig = plt.figure(figsize=(24, 6))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_xlabel('Iterations', fontsize=16)\n",
    "ax1.set_ylabel('Function Value', fontsize=16)\n",
    "ax1.plot(its, train_avg2, color=jungle_green, label='IndeCateR')\n",
    "ax1.fill_between(its, train_avg2 - train_std2, train_avg2 + train_std2, alpha=0.33, color=jungle_green)\n",
    "ax1.plot(its, train_avg, color=pink, label='RLOO-800', linestyle='dashed')\n",
    "ax1.fill_between(its, train_avg - train_std, train_avg + train_std, alpha=0.33, color=pink, linestyle='dashed')\n",
    "ax1.plot(its, train_avg4, color=celadon_blue, label='RLOO-2', linestyle='dashed')\n",
    "ax1.fill_between(its, train_avg4 - train_std4, train_avg4 + train_std4, alpha=0.33, color=celadon_blue, linestyle='dashed')\n",
    "ax1.plot(its, train_avg3, color=orange_red, label='GS', linestyle='dotted')\n",
    "ax1.fill_between(its, train_avg3 - train_std3, train_avg3 + train_std3, alpha=0.33, color=orange_red, linestyle='dotted')\n",
    "ax1.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.set_xlabel('Time (s)', fontsize=16)\n",
    "ax2.set_ylabel('Function Value', fontsize=16)\n",
    "ax2.plot(its2, train_avg2, color=jungle_green, label='IndeCateR')\n",
    "ax2.fill_between(its2, train_avg2 - train_std2, train_avg2 + train_std2, alpha=0.33, color=jungle_green)\n",
    "ax2.plot(its1, train_avg, color=pink, label='RLOO-F', linestyle='dashed')\n",
    "ax2.fill_between(its1, train_avg - train_std, train_avg + train_std, alpha=0.33, color=pink, linestyle='dashed')\n",
    "ax2.plot(its4, train_avg4, color=celadon_blue, label='RLOO-S', linestyle='dashed')\n",
    "ax2.fill_between(its4, train_avg4 - train_std4, train_avg4 + train_std4, alpha=0.33, color=celadon_blue, linestyle='dashed')\n",
    "ax2.plot(its3, train_avg3, color=orange_red, label='GS-F', linestyle='dotted')\n",
    "ax2.fill_between(its3, train_avg3 - train_std3, train_avg3 + train_std3, alpha=0.33, color=orange_red, linestyle='dotted')\n",
    "ax2.spines[['right', 'top']].set_visible(False)\n",
    "ax2.set_xlim(xmax=min_time)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.set_xlabel('Iterations', fontsize=16)\n",
    "ax3.set_ylabel('Gradient Variance', fontsize=16)\n",
    "ax3.plot(its, var_avg2, color=jungle_green, label='IndeCateR')\n",
    "ax3.fill_between(its, var_avg2 - var_std2, var_avg2 + var_std2, alpha=0.33, color=jungle_green)\n",
    "ax3.plot(its, var_avg, color=pink, label='RLOO-F', linestyle='dashed')\n",
    "ax3.fill_between(its, var_avg - var_std, var_avg + var_std, alpha=0.33, color=pink, linestyle='dashed')\n",
    "ax3.plot(its, var_avg4, color=celadon_blue, label='RLOO-S', linestyle='dashed')\n",
    "ax3.fill_between(its, var_avg4 - var_std4, var_avg4 + var_std4, alpha=0.33, color=celadon_blue, linestyle='dashed')\n",
    "ax3.plot(its, var_avg3, color=orange_red, label='GS-F', linestyle='dotted')\n",
    "ax3.fill_between(its, var_avg3 - var_std3, var_avg3 + var_std3, alpha=0.33, color=orange_red, linestyle='dotted')\n",
    "ax3.spines[['right', 'top']].set_visible(False)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "leg = plt.legend(frameon=False, fontsize=24, loc='upper center', bbox_to_anchor=(-.8, 1.2), ncol=4)\n",
    "for line in leg.get_lines():\n",
    "    line.set_linewidth(4)\n",
    "plt.plot()\n",
    "plt.savefig(f\"/cw/dtaijupiter/NoCsBack/dtai/lennert/CATSCH/synthetic/plots/synthetic_optimisation.pdf\", dpi=300, bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
